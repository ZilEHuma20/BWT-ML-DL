# -*- coding: utf-8 -*-
"""Pakwheels

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UCYaejRSeuFRZGcqpbD3Cg0WzgxrMF3R
"""

from google.colab import files
import pandas as pd

# Upload the file
uploaded = files.upload()

# Now load the dataset into a DataFrame
import io
data = pd.read_csv(io.BytesIO(uploaded['pakwheels_used_cars.csv']))  # Adjust the filename accordingly

# Display the first few rows of the dataset
print(data.head())

# Import the necessary libraries
from google.colab import files
import pandas as pd
import io

# Step 1: Upload the file
uploaded = files.upload()  # This will prompt you to upload the file

# Step 2: Print the names of the uploaded files
print("Uploaded files:", uploaded.keys())

# Step 3: Load the dataset into a DataFrame using the correct filename
# Ensure you are using the actual filename you see in the output above
if uploaded:  # Check if any files were uploaded
    data_file_name = list(uploaded.keys())[0]  # Dynamically get the first uploaded file name
    data = pd.read_csv(io.BytesIO(uploaded[data_file_name]))

    # Display the first few rows of the dataset
    print("First few rows of the dataset:")
    print(data.head())

    # Check for missing values
    missing_values = data.isnull().sum()
    missing_percentage = (missing_values / len(data)) * 100

    # Display missing values and their percentages
    print("\nMissing Values Count:")
    print(missing_values)
    print("\nPercentage of Missing Values:")
    print(missing_percentage)

    # Decide on how to handle missing values
    # Drop columns with more than 50% missing values
    columns_to_drop = missing_percentage[missing_percentage > 50].index
    data = data.drop(columns=columns_to_drop)

    # Impute missing values with mean for numerical columns and mode for categorical columns
    for column in data.columns:
        if data[column].dtype == 'object':  # Categorical column
            data[column].fillna(data[column].mode()[0], inplace=True)
        else:  # Numerical column
            data[column].fillna(data[column].mean(), inplace=True)

    # Verify there are no missing values left
    print("\nMissing Values After Handling:")
    print(data.isnull().sum())

    # Display the cleaned dataset
    print("\nCleaned Dataset:")
    print(data.head())
else:
    print("No files uploaded. Please try uploading the file again.")

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler
import pandas as pd

# Assume data is already loaded and cleaned from previous steps
# Display initial types of features
print("Initial Data Types:\n", data.dtypes)

# Step 1: Identify Categorical and Numerical columns
categorical_cols = data.select_dtypes(include=['object']).columns
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns

# Encoding Categorical Variables
# For Label Encoding (if applicable):
label_encoder = LabelEncoder()

# Optionally, apply Label Encoding for ordinal categorical data
for col in categorical_cols:
    if data[col].nunique() <= 10:  # Example condition; adjust as per need
        data[col] = label_encoder.fit_transform(data[col])

# For One-Hot Encoding:
data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)  # drop_first avoids multicollinearity

# Normalizing/Standardizing Numerical Features
# Standardization
scaler = StandardScaler()
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

# # Alternatively, Normalization (Uncomment if Normalization is preferred)
# normalizer = MinMaxScaler()
# data[numerical_cols] = normalizer.fit_transform(data[numerical_cols])

# Display the transformed dataset
print("\nTransformed Data Types:\n", data.dtypes)
print("\nTransformed Dataset:\n", data.head())

from sklearn.model_selection import train_test_split

# Assuming 'data' is the transformed DataFrame and 'target' is the target column name
# Replace 'target_column' with the actual name of the target column in your dataset
target_column = 'price'  # Change this to your actual target column

# Separate features (X) and target (y)
X = data.drop(columns=[target_column])  # Features
y = data[target_column]  # Target

# Split the data into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets
print("Training Features Shape:", X_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Training Labels Shape:", y_train.shape)
print("Testing Labels Shape:", y_test.shape)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/pakwheels_used_cars (1).csv')

# Handle missing values
# Fill numerical columns with the median
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())

# Fill categorical columns with the mode
categorical_cols = data.select_dtypes(include=['object']).columns
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

# Step 1: Summary Statistics
print("Summary Statistics:")
print(data.describe(include='all'))

# Step 2: Correlation Matrix (for numerical features only)
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
correlation_matrix = data[numerical_cols].corr()

print("\nCorrelation Matrix:")
print(correlation_matrix)

# Visualize the Correlation Matrix with a Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Step 3: Visualize Distributions of Numerical Features
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Step 4: Box Plots to Identify Outliers
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data[col])
    plt.title(f'Box Plot of {col}')
    plt.xlabel(col)
    plt.show()

# Step 5: Pairplot to Visualize Relationships Between Features
plt.figure(figsize=(10, 10))
sns.pairplot(data[numerical_cols])
plt.title('Pairplot of Numerical Features')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/pakwheels_used_cars (1).csv')

# Handle missing values (same as before)
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())
categorical_cols = data.select_dtypes(include=['object']).columns
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

# Visualization 1: Histograms for each numerical feature
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}', fontsize=14)
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

# Visualization 2: Scatter Plots for pairs of numerical features
# We'll limit to a few pairs for clarity
pairs = [('price', 'mileage'), ('engine_cc', 'price'), ('year', 'price')]
for x, y in pairs:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=data[x], y=data[y])
    plt.title(f'Scatter Plot of {x} vs {y}', fontsize=14)
    plt.xlabel(x)
    plt.ylabel(y)
    plt.grid(True)
    plt.show()

# Visualization 3: Box Plots to Identify Outliers in Numerical Features
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data[col])
    plt.title(f'Box Plot of {col}', fontsize=14)
    plt.xlabel(col)
    plt.grid(True)
    plt.show()

# Visualization 4: Correlation Heatmap for Numerical Features
correlation_matrix = data[numerical_cols].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv('/content/pakwheels_used_cars (1).csv')

# Step 1: Data Preprocessing (Handling missing values)
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

# Fill missing values
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

# Step 2: Data Visualization
# Visualization 1: Histograms for each numerical feature
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}', fontsize=14)
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

# Visualization 2: Scatter Plots for pairs of numerical features
pairs = [('price', 'mileage'), ('engine_cc', 'price'), ('year', 'price')]
for x, y in pairs:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=data[x], y=data[y])
    plt.title(f'Scatter Plot of {x} vs {y}', fontsize=14)
    plt.xlabel(x)
    plt.ylabel(y)
    plt.grid(True)
    plt.show()

# Visualization 3: Box Plots to Identify Outliers
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data[col])
    plt.title(f'Box Plot of {col}', fontsize=14)
    plt.xlabel(col)
    plt.grid(True)
    plt.show()

# Visualization 4: Correlation Heatmap for Numerical Features
correlation_matrix = data[numerical_cols].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

# Step 3: Regression Modeling

# Defining the target (price) and features
X = data.drop('price', axis=1)  # Features
y = data['price']  # Target: Price column

# One-Hot Encoding for Categorical Variables
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Applying Regression Models

# Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

# Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

# Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Step 4: Model Evaluation

def evaluate_regression(y_test, y_pred, model_name):
    print(f"Evaluation for {model_name}:")
    print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
    print("R-squared:", r2_score(y_test, y_pred))
    print()

# Evaluate all models
evaluate_regression(y_test, y_pred_lr, "Linear Regression")
evaluate_regression(y_test, y_pred_dt, "Decision Tree Regressor")
evaluate_regression(y_test, y_pred_rf, "Random Forest Regressor")

# Optional: Visualizing the predictions vs actual prices for one of the models (e.g., Linear Regression)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_lr, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--', color='black')
plt.title('True vs Predicted Prices (Linear Regression)')
plt.xlabel('True Prices')
plt.ylabel('Predicted Prices')
plt.grid(True)
plt.show()