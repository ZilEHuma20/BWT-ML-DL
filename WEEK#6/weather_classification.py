# -*- coding: utf-8 -*-
"""weather classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t0307Vg8l--1ytVyATpMBP_v5AyeIrTa
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the dataset
file_path = '/content/weather_classification_data.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset
print("Initial Data Preview:")
print(data.head())

# Check for missing values
print("\nMissing Values Summary:")
print(data.isnull().sum())

# Handle missing values (if any) - Example: Fill with mean for numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
for col in numerical_columns:
    data[col].fillna(data[col].mean(), inplace=True)

# Handle missing values for categorical columns - Example: Fill with mode
categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    data[col].fillna(data[col].mode()[0], inplace=True)

# Encode categorical variables
label_encoders = {}
for col in categorical_columns:
    label_encoders[col] = LabelEncoder()
    data[col] = label_encoders[col].fit_transform(data[col])

# Standardize numerical features
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display the cleaned and preprocessed data
print("\nCleaned and Preprocessed Data Preview:")
print(data.head())

# Save the cleaned data to a new CSV (optional)
cleaned_file_path = '/content/weather_classification_data.csv'
data.to_csv(cleaned_file_path, index=False)
print(f"\nCleaned data saved to {cleaned_file_path}")

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the dataset
file_path = '/content/weather_classification_data.csv'
data = pd.read_csv(file_path)

# Identify missing values
print("\nMissing Values Summary:")
missing_values = data.isnull().sum()
print(missing_values)

# Handling Missing Values
# 1. For Numerical Columns: Fill missing values with the mean
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
for col in numerical_columns:
    if data[col].isnull().sum() > 0:
        data[col].fillna(data[col].mean(), inplace=True)
        print(f"Filled missing values in {col} with mean: {data[col].mean()}")

# 2. For Categorical Columns: Fill missing values with the mode
categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    if data[col].isnull().sum() > 0:
        data[col].fillna(data[col].mode()[0], inplace=True)
        print(f"Filled missing values in {col} with mode: {data[col].mode()[0]}")

# Encode categorical variables after filling missing values
label_encoders = {}
for col in categorical_columns:
    label_encoders[col] = LabelEncoder()
    data[col] = label_encoders[col].fit_transform(data[col])

# Standardize numerical features after handling missing values
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display the cleaned and preprocessed data
print("\nCleaned and Preprocessed Data Preview:")
print(data.head())

# Save the cleaned data to a new CSV (in the current directory or accessible directory)
cleaned_file_path = 'cleaned_weather_data.csv'  # Save in the current working directory
data.to_csv(cleaned_file_path, index=False)
print(f"\nCleaned data saved to {cleaned_file_path}")

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler

# Load the dataset
file_path = '/content/weather_classification_data.csv'
data = pd.read_csv(file_path)

# Display initial data preview
print("Initial Data Preview:")
print(data.head())

# Identify numerical and categorical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = data.select_dtypes(include=['object']).columns

# Convert Categorical Variables to Numerical
# 1. For Label Encoding (useful for binary or ordinal categories)
label_encoders = {}
for col in categorical_columns:
    if data[col].nunique() == 2:  # Example: Use label encoding for binary categories
        label_encoders[col] = LabelEncoder()
        data[col] = label_encoders[col].fit_transform(data[col])

# 2. For One-Hot Encoding (useful for nominal categories)
data = pd.get_dummies(data, columns=[col for col in categorical_columns if col not in label_encoders], drop_first=True)

# Standardize Numerical Features
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display the transformed data
print("\nTransformed Data Preview:")
print(data.head())

# Save the transformed data to a new CSV (optional)
transformed_file_path = 'transformed_weather_data.csv'  # Save in the current working directory
data.to_csv(transformed_file_path, index=False)
print(f"\nTransformed data saved to {transformed_file_path}")

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the transformed dataset
file_path = 'transformed_weather_data.csv'  # Assuming you have already transformed and saved the data
data = pd.read_csv(file_path)

# Separate features (X) and target variable (y)
# Assuming 'Weather Type' is the target variable for classification
# Replace 'Weather Type' with the correct target variable name if different
X = data.drop('Weather Type', axis=1)  # Features
y = data['Weather Type']  # Target

# Split the data into training and testing sets with an 80-20 split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of the resulting datasets
print(f"Training Features Shape: {X_train.shape}")
print(f"Testing Features Shape: {X_test.shape}")
print(f"Training Labels Shape: {y_train.shape}")
print(f"Testing Labels Shape: {y_test.shape}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/weather_classification_data.csv')

# Display basic summary statistics
print("Summary Statistics:")
print(data.describe(include='all'))

# Display correlation matrix for numerical features
print("\nCorrelation Matrix:")
correlation_matrix = data.corr(numeric_only=True)  # 'numeric_only=True' handles future versions of pandas
print(correlation_matrix)

# Plot correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

# Identify the numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Adjust the figure size and number of subplots based on the number of columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot((len(numerical_columns) + 2) // 3, 3, i)  # Adjust rows based on number of numerical columns
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')

plt.tight_layout()
plt.show()

# Visualize distributions of categorical features
categorical_columns = data.select_dtypes(include=['object', 'category']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(categorical_columns, 1):
    plt.subplot(3, 3, i)
    sns.countplot(data=data, x=col)
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Identify the numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Adjust the figure size and number of subplots based on the number of columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot((len(numerical_columns) + 2) // 3, 3, i)  # Adjust rows based on number of numerical columns
    sns.boxplot(data=data[col])
    plt.title(f'Box Plot of {col}')

plt.tight_layout()
plt.show()
# Pair plots to explore relationships between numerical features
sns.pairplot(data[numerical_columns])
plt.suptitle('Pair Plot of Numerical Features', y=1.02)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('/content/weather_classification_data.csv')

# Identify numerical and categorical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = data.select_dtypes(include=['object', 'category']).columns

# 1. Histograms - Distribution of Individual Features
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot((len(numerical_columns) + 2) // 3, 3, i)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
plt.tight_layout()
plt.show()

# 2. Scatter Plots - Relationships between Pairs of Features
# Pairwise scatter plots for the first 4 numerical features
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns[:4], 1):  # Limit to the first 4 numerical columns
    for j, col2 in enumerate(numerical_columns[:4], 1):
        plt.subplot(4, 4, (i-1)*4 + j)
        plt.scatter(data[col], data[col2])
        if i == 4:
            plt.xlabel(col2)
        if j == 1:
            plt.ylabel(col)
        plt.title(f'{col} vs {col2}')
plt.tight_layout()
plt.show()

# 3. Box Plots - Identifying Outliers and Spread
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot((len(numerical_columns) + 2) // 3, 3, i)
    sns.boxplot(data=data[col])
    plt.title(f'Box Plot of {col}')
    plt.xlabel(col)
plt.tight_layout()
plt.show()

# 4. Correlation Heatmap - Correlations between Features
correlation_matrix = data.corr(numeric_only=True)  # Only works with numeric columns
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/weather_classification_data.csv')

# Step 1: Inspect the unique values of 'Weather Type' column
print("Unique Weather Types: ", data['Weather Type'].unique())  # This will show the unique categories

# Step 2: Preprocessing
# One-hot encode the categorical variables
data_encoded = pd.get_dummies(data, drop_first=True)

# Check the columns after encoding
print("Columns after encoding: ", data_encoded.columns)

# Select the target column dynamically based on the encoded columns
# Print available columns for Weather Type and choose one that exists
weather_type_columns = [col for col in data_encoded.columns if 'Weather Type' in col]
print("Available Weather Type columns: ", weather_type_columns)

# Select a target weather type from the available encoded columns (you can change this based on the output)
y = data_encoded[weather_type_columns[0]]  # Automatically pick the first available Weather Type column

# Separate features (X) by dropping all 'Weather Type' columns
X = data_encoded.drop(weather_type_columns, axis=1)

# Step 3: Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Feature scaling (standardizing data for algorithms like Logistic Regression)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Model Selection and Training

# 5.1 Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)

# 5.2 Decision Tree Classifier
tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)
y_pred_tree = tree_clf.predict(X_test)

# 5.3 Random Forest Classifier
forest_clf = RandomForestClassifier(random_state=42)
forest_clf.fit(X_train, y_train)
y_pred_forest = forest_clf.predict(X_test)

# Step 6: Model Evaluation
print("Logistic Regression Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}")
print(classification_report(y_test, y_pred_log_reg))

print("\nDecision Tree Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_tree):.4f}")
print(classification_report(y_test, y_pred_tree))

print("\nRandom Forest Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_forest):.4f}")
print(classification_report(y_test, y_pred_forest))

# Step 7: Confusion Matrix Visualization
models = [('Logistic Regression', y_pred_log_reg),
          ('Decision Tree', y_pred_tree),
          ('Random Forest', y_pred_forest)]

for model_name, y_pred in models:
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/weather_classification_data.csv')

# Step 1: Preprocessing
# One-hot encode the categorical variables
data_encoded = pd.get_dummies(data, drop_first=True)

# Get the list of encoded weather types
weather_type_columns = [col for col in data_encoded.columns if 'Weather Type' in col]

# For demonstration, we are picking the first available weather type
y = data_encoded[weather_type_columns[0]]  # Replace with a specific weather type column if needed
X = data_encoded.drop(weather_type_columns, axis=1)

# Step 2: Split the data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 4: Model Training
# Initialize models
log_reg = LogisticRegression(max_iter=1000)
tree_clf = DecisionTreeClassifier(random_state=42)
forest_clf = RandomForestClassifier(random_state=42)

# Train and predict with each model
models = {
    "Logistic Regression": log_reg,
    "Decision Tree": tree_clf,
    "Random Forest": forest_clf
}

model_performance = {}

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate accuracy, precision, recall, F1-score
    report = classification_report(y_test, y_pred, output_dict=True)
    accuracy = accuracy_score(y_test, y_pred)

    # Save model performance
    model_performance[model_name] = {
        "Accuracy": accuracy,
        "Precision": report['1']['precision'],  # '1' represents the positive class
        "Recall": report['1']['recall'],
        "F1-score": report['1']['f1-score']
    }

# Step 5: Create a DataFrame for comparison
performance_df = pd.DataFrame(model_performance).T  # Transpose for better readability

# Step 6: Display the performance metrics table
print(performance_df)

# Step 7: Visualize performance comparison
# Plot bar graphs for each metric
performance_df.plot(kind='bar', figsize=(10, 6))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc="best")
plt.tight_layout()
plt.show()

# Step 8: Confusion Matrix Visualization
for model_name, model in models.items():
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Step 9: Discussion
best_model = performance_df["Accuracy"].idxmax()
print(f"The best performing model based on accuracy is: {best_model}")